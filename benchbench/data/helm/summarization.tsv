Model	Mean win rate	CNN/DailyMail - SummaC	CNN/DailyMail - QAFactEval	CNN/DailyMail - BERTScore (F1)	CNN/DailyMail - Coverage	CNN/DailyMail - Density	CNN/DailyMail - Compression	CNN/DailyMail - HumanEval-faithfulness	CNN/DailyMail - HumanEval-relevance	CNN/DailyMail - HumanEval-coherence	XSUM - SummaC	XSUM - QAFactEval	XSUM - BERTScore (F1)	XSUM - Coverage	XSUM - Density	XSUM - Compression	XSUM - HumanEval-faithfulness	XSUM - HumanEval-relevance	XSUM - HumanEval-coherence
TNLG v2 (530B)	0.757	0.573	-	0.316	0.977	26.968	10.317	-	-	-	-0.281	-	0.473	0.774	2.322	15.776	-	-	-
Luminous Supreme (70B)	0.717	0.552	-	0.28	0.939	33.625	9.298	-	-	-	-0.241	-	0.444	0.807	3.08	16.97	-	-	-
Cohere xlarge v20221108 (52.4B)	0.704	0.514	-	0.286	0.971	44.772	8.026	-	-	-	-0.258	-	0.451	0.798	3.009	17.188	-	-	-
J1-Grande v2 beta (17B)	0.678	0.552	-	0.29	0.973	24.032	11.659	-	-	-	-0.282	-	0.454	0.786	2.816	16.857	-	-	-
Cohere Command beta (52.4B)	0.678	0.415	-	0.318	0.979	32.165	9.156	-	-	-	-0.271	-	0.459	0.793	2.548	16.937	-	-	-
Jurassic-2 Grande (17B)	0.671	0.503	-	0.299	0.96	22.305	11.399	-	-	-	-0.289	-	0.475	0.766	2.36	17.045	-	-	-
J1-Grande v1 (17B)	0.669	0.539	4.81	0.275	0.973	41.027	9.888	-	-	-	-0.272	3.447	0.429	0.783	2.64	19.012	-	-	-
J1-Large v1 (7.5B)	0.65	0.512	4.716	0.248	0.977	71.654	7.632	-	-	-	-0.239	3.675	0.4	0.808	3.757	18.133	-	-	-
text-babbage-001	0.646	0.378	4.676	0.282	0.972	45.948	5.291	-	-	-	-0.057	4.33	0.281	0.885	8.487	11.856	-	-	-
Jurassic-2 Jumbo (178B)	0.645	0.489	-	0.313	0.957	15.317	12.304	-	-	-	-0.32	-	0.489	0.755	2.145	16.589	-	-	-
text-davinci-002	0.641	0.353	4.635	0.321	0.946	15.995	8.818	0.999	4.435	4.371	-0.273	3.007	0.43	0.801	2.872	14.07	0.849	4.41	4.685
text-curie-001	0.617	0.291	4.616	0.306	0.961	26.1	6.829	0.967	4.587	4.243	-0.185	3.459	0.354	0.839	4.008	12.98	0.991	4.068	4.321
TNLG v2 (6.7B)	0.612	0.493	-	0.282	0.976	48.951	9.598	-	-	-	-0.203	-	0.385	0.793	3.286	18.428	-	-	-
OPT (175B)	0.593	0.202	4.67	0.276	0.933	31.307	9.8	1	4.378	3.233	-0.253	3.523	0.46	0.793	2.732	16.792	0.798	4.3	4.891
J1-Jumbo v1 (178B)	0.587	0.515	4.697	0.278	0.976	53.93	9.579	-	-	-	-0.287	3.182	0.435	0.784	2.63	16.862	-	-	-
Cohere Command beta (6.1B)	0.579	0.331	-	0.296	0.975	31.707	9.688	-	-	-	-0.239	-	0.418	0.824	2.793	18.017	-	-	-
OPT (66B)	0.579	0.197	4.735	0.256	0.92	41.595	9.759	-	-	-	-0.189	3.324	0.417	0.817	3.899	18.414	-	-	-
Cohere large v20220720 (13.1B)	0.576	0.5	4.763	0.246	0.946	37.733	11.27	-	-	-	-0.189	2.889	0.398	0.823	3.599	20.712	-	-	-
Jurassic-2 Large (7.5B)	0.572	0.496	-	0.271	0.963	25.251	11.503	-	-	-	-0.278	-	0.45	0.782	2.659	18.03	-	-	-
Luminous Extended (30B)	0.566	0.481	-	0.255	0.925	41.619	9.039	-	-	-	-0.225	-	0.423	0.818	3.507	17.376	-	-	-
GPT-J (6B)	0.549	0.208	4.704	0.247	0.948	48.284	9.864	-	-	-	-0.198	3.813	0.381	0.829	4.043	17.942	-	-	-
Cohere xlarge v20220609 (52.4B)	0.546	0.469	4.683	0.264	0.945	49.713	9.072	0.993	4.539	3.69	-0.253	2.981	0.434	0.8	2.945	18.422	0.661	4.239	4.825
Anthropic-LM v4-s3 (52B)	0.531	0.492	4.692	0.326	0.96	10.832	11.89	0.667	4	2.667	-0.271	3.066	0.437	0.808	2.691	15.182	0.778	4.398	4.898
text-davinci-003	0.526	0.359	-	0.342	0.956	7.545	9.389	-	-	-	-0.301	-	0.411	0.822	2.63	10.932	-	-	-
Cohere medium v20221108 (6.1B)	0.507	0.359	-	0.218	0.899	24.344	11.42	-	-	-	-0.171	-	0.384	0.842	3.815	19.703	-	-	-
text-ada-001	0.486	0.223	3.369	0.247	0.929	31.424	5.461	-	-	-	-0.102	4.929	0.245	0.847	7.626	13.08	-	-	-
GLM (130B)	0.471	0.566	-	0.288	0.972	30.259	8.687	0.963	4.167	3.463	-0.206	-	0.427	0.817	4.041	16.25	0.763	3.843	4.25
GPT-NeoX (20B)	0.446	0.165	4.69	0.226	0.91	37.149	9.676	-	-	-	-0.208	3.303	0.391	0.825	3.371	18.238	-	-	-
Cohere medium v20220720 (6.1B)	0.431	0.229	4.664	0.115	0.799	22.176	13.154	-	-	-	-0.159	3.223	0.367	0.847	4.754	19.748	-	-	-
Luminous Base (13B)	0.421	0.32	-	0.188	0.834	35.663	9.346	-	-	-	-0.213	-	0.394	0.834	4.393	17.535	-	-	-
davinci (175B)	0.36	0.321	4.062	0.182	0.873	17.914	9.843	0.953	4.501	3.863	-0.267	2.338	0.318	0.751	3.351	14.08	0.829	4.075	3.398
curie (6.7B)	0.325	0.354	4.204	0.089	0.89	23.472	9.495	0.287	1.933	1.767	-0.143	3.922	0.313	0.815	5.57	17.018	0.924	3.573	4.166
Cohere small v20220720 (410M)	0.292	0.054	2.638	0.026	0.744	25.238	13.243	-	-	-	0.028	3.094	0.195	0.863	10.557	17.551	-	-	-
BLOOM (176B)	0.291	-0.02	4.665	0.08	0.71	32.013	5.252	-	-	-	-0.35	4.778	0.059	0.515	1.764	8.934	-	-	-
ada (350M)	0.231	0.169	3.742	0.026	0.773	36.596	12.07	-	-	-	-0.115	0.009	-0.232	0.407	2.653	8.023	-	-	-
babbage (1.3B)	0.196	0.194	3.207	-0.129	0.606	43.534	6.733	-	-	-	-0.188	0.195	0.02	0.604	4.386	11.716	-	-	-
UL2 (20B)	0.118	-0.27	-	-0.121	0.72	5.044	7.186	-	-	-	-0.275	-	0.072	0.643	3.208	7.853	-	-	-
T5 (11B)	0.112	-0.122	-	-0.17	0.555	2.698	19.248	-	-	-	-0.258	-	-0.315	0.355	0.831	16.544	-	-	-
YaLM (100B)	0.045	-0.322	-	-0.145	0.541	1.09	6.936	-	-	-	-0.347	1.176	0.031	0.567	1.041	9.951	-	-	-
T0pp (11B)	-	-0.044	-	0.155	0.841	8.588	8.274	-	-	-	-0.3	-	0.097	0.579	1.684	11.178	-	-	-
Pythia (6.9B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Pythia (12B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
LLaMA (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
LLaMA (13B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
LLaMA (30B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
LLaMA (65B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Llama 2 (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Llama 2 (13B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Llama 2 (70B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Alpaca (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Vicuna v1.3 (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Vicuna v1.3 (13B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Mistral v0.1 (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
gpt-3.5-turbo-0301	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
gpt-3.5-turbo-0613	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
RedPajama-INCITE-Base-v1 (3B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
RedPajama-INCITE-Instruct-v1 (3B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
RedPajama-INCITE-Base (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
RedPajama-INCITE-Instruct (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
MPT (30B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
MPT-Instruct (30B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Falcon (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Falcon-Instruct (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Falcon (40B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Falcon-Instruct (40B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
InstructPalmyra (30B)	-	-	-	-	0.972	28.97	7.901	-	-	-	-	-	-	0.844	3.441	15.707	-	-	-
Palmyra X (43B)	-	-	-	-	0.291	2.35	3.117	-	-	-	-	-	-	0.775	2.466	14.252	-	-	-

