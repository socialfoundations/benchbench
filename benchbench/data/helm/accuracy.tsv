Model/adapter	Mean win rate	MMLU - EM	BoolQ - EM	NarrativeQA - F1	NaturalQuestions (closed-book) - F1	NaturalQuestions (open-book) - F1	QuAC - F1	HellaSwag - EM	OpenbookQA - EM	TruthfulQA - EM	MS MARCO (regular) - RR@10	MS MARCO (TREC) - NDCG@10	CNN/DailyMail - ROUGE-2	XSUM - ROUGE-2	IMDB - EM	CivilComments - EM	RAFT - EM
Llama 2 (70B)	0.944	0.582	0.886	0.77	0.458	0.674	0.484	-	-	0.554	-	-	-	-	0.961	0.652	0.727
LLaMA (65B)	0.908	0.584	0.871	0.755	0.431	0.672	0.401	-	-	0.508	-	-	-	-	0.962	0.655	0.702
text-davinci-002	0.905	0.568	0.877	0.727	0.383	0.713	0.445	0.815	0.594	0.61	0.421	0.664	0.153	0.144	0.948	0.668	0.733
Mistral v0.1 (7B)	0.884	0.572	0.874	0.716	0.365	0.687	0.423	-	-	0.422	-	-	-	-	0.962	0.624	0.707
Cohere Command beta (52.4B)	0.874	0.452	0.856	0.752	0.372	0.76	0.432	0.811	0.582	0.269	0.472	0.762	0.161	0.152	0.96	0.601	0.667
text-davinci-003	0.872	0.569	0.881	0.727	0.406	0.77	0.525	0.822	0.646	0.593	0.368	0.644	0.156	0.124	0.848	0.684	0.759
Jurassic-2 Jumbo (178B)	0.824	0.48	0.829	0.733	0.385	0.669	0.435	0.788	0.558	0.437	0.398	0.661	0.149	0.182	0.938	0.57	0.746
Llama 2 (13B)	0.823	0.507	0.811	0.744	0.376	0.637	0.424	-	-	0.33	-	-	-	-	0.962	0.588	0.707
TNLG v2 (530B)	0.787	0.469	0.809	0.722	0.384	0.642	0.39	0.799	0.562	0.251	0.377	0.643	0.161	0.169	0.941	0.601	0.679
gpt-3.5-turbo-0613	0.783	0.391	0.87	0.625	0.348	0.675	0.485	-	-	0.339	-	-	-	-	0.943	0.696	0.748
LLaMA (30B)	0.781	0.531	0.861	0.752	0.408	0.666	0.39	-	-	0.344	-	-	-	-	0.927	0.549	0.752
Anthropic-LM v4-s3 (52B)	0.78	0.481	0.815	0.728	0.288	0.686	0.431	0.807	0.558	0.368	-	-	0.154	0.134	0.934	0.61	0.699
gpt-3.5-turbo-0301	0.76	0.59	0.74	0.663	0.39	0.624	0.512	-	-	0.609	-	-	-	-	0.899	0.674	0.768
Jurassic-2 Grande (17B)	0.743	0.475	0.826	0.737	0.356	0.639	0.418	0.781	0.542	0.348	0.293	0.514	0.144	0.167	0.938	0.547	0.712
Palmyra X (43B)	0.732	0.609	0.896	0.742	0.413	-	0.473	-	-	0.616	-	-	0.049	0.149	0.935	0.008	0.701
Falcon (40B)	0.729	0.509	0.819	0.673	0.392	0.675	0.307	-	-	0.353	-	-	-	-	0.959	0.552	0.661
Falcon-Instruct (40B)	0.727	0.497	0.829	0.625	0.377	0.666	0.371	-	-	0.384	-	-	-	-	0.959	0.603	0.586
MPT-Instruct (30B)	0.716	0.444	0.85	0.733	0.304	0.697	0.327	-	-	0.234	-	-	-	-	0.956	0.573	0.68
MPT (30B)	0.714	0.437	0.704	0.732	0.347	0.673	0.393	-	-	0.231	-	-	-	-	0.959	0.599	0.723
J1-Grande v2 beta (17B)	0.706	0.445	0.812	0.725	0.337	0.625	0.392	0.764	0.56	0.306	0.285	0.46	0.146	0.152	0.957	0.546	0.679
Vicuna v1.3 (13B)	0.706	0.462	0.808	0.691	0.346	0.686	0.403	-	-	0.385	-	-	-	-	0.762	0.645	0.657
Cohere Command beta (6.1B)	0.675	0.406	0.798	0.709	0.229	0.717	0.375	0.752	0.55	0.203	0.434	0.709	0.153	0.122	0.961	0.54	0.634
Cohere xlarge v20221108 (52.4B)	0.664	0.382	0.762	0.672	0.361	0.628	0.374	0.81	0.588	0.169	0.315	0.55	0.153	0.153	0.956	0.524	0.624
Luminous Supreme (70B)	0.662	0.38	0.775	0.711	0.293	0.649	0.37	-	-	0.222	-	-	0.15	0.136	0.959	0.562	0.653
Vicuna v1.3 (7B)	0.625	0.434	0.76	0.643	0.287	0.634	0.392	-	-	0.292	-	-	-	-	0.916	0.62	0.693
OPT (175B)	0.609	0.318	0.793	0.671	0.297	0.615	0.36	0.791	0.586	0.25	0.288	0.448	0.146	0.155	0.947	0.505	0.606
Llama 2 (7B)	0.607	0.431	0.762	0.691	0.337	0.611	0.406	-	-	0.272	-	-	-	-	0.907	0.562	0.643
LLaMA (13B)	0.595	0.422	0.714	0.711	0.346	0.614	0.347	-	-	0.324	-	-	-	-	0.928	0.6	0.643
InstructPalmyra (30B)	0.568	0.403	0.751	0.496	0.33	0.682	0.433	-	-	0.185	-	-	0.152	0.104	0.94	0.555	0.652
Cohere xlarge v20220609 (52.4B)	0.56	0.353	0.718	0.65	0.312	0.595	0.361	0.811	0.55	0.198	0.273	0.459	0.144	0.129	0.956	0.532	0.633
Jurassic-2 Large (7.5B)	0.553	0.339	0.742	-	0.274	0.589	-	0.729	0.53	0.245	0.247	0.464	0.136	0.142	0.956	0.57	0.622
davinci (175B)	0.538	0.422	0.722	0.687	0.329	0.625	0.36	0.775	0.586	0.194	0.211	0.378	0.127	0.126	0.933	0.532	0.642
LLaMA (7B)	0.533	0.321	0.756	0.669	0.297	0.589	0.338	-	-	0.28	-	-	-	-	0.947	0.563	0.573
RedPajama-INCITE-Instruct (7B)	0.524	0.363	0.705	0.638	0.232	0.659	0.26	-	-	0.243	-	-	-	-	0.927	0.664	0.695
J1-Jumbo v1 (178B)	0.517	0.259	0.776	0.695	0.293	0.595	0.358	0.765	0.534	0.175	0.21	0.363	0.144	0.129	0.943	0.553	0.681
GLM (130B)	0.512	0.344	0.784	0.706	0.148	0.642	0.272	-	-	0.218	-	-	0.154	0.132	0.955	0.5	0.598
Luminous Extended (30B)	0.485	0.321	0.767	0.665	0.254	0.609	0.349	-	-	0.221	-	-	0.139	0.124	0.947	0.524	0.523
OPT (66B)	0.448	0.276	0.76	0.638	0.258	0.596	0.357	0.745	0.534	0.201	0.237	0.482	0.136	0.126	0.917	0.506	0.557
BLOOM (176B)	0.446	0.299	0.704	0.662	0.216	0.621	0.361	0.744	0.534	0.205	0.236	0.386	0.08	0.03	0.945	0.62	0.592
J1-Grande v1 (17B)	0.433	0.27	0.722	0.672	0.233	0.578	0.362	0.739	0.52	0.193	0.161	0.341	0.143	0.122	0.953	0.529	0.658
Alpaca (7B)	0.381	0.385	0.778	0.396	0.266	0.592	0.27	-	-	0.243	-	-	-	-	0.738	0.566	0.486
Falcon (7B)	0.378	0.286	0.753	0.621	0.285	0.579	0.332	-	-	0.234	-	-	-	-	0.836	0.514	0.602
RedPajama-INCITE-Base (7B)	0.378	0.302	0.713	0.617	0.25	0.586	0.336	-	-	0.205	-	-	-	-	0.752	0.547	0.648
Cohere large v20220720 (13.1B)	0.372	0.324	0.725	0.625	0.232	0.573	0.338	0.736	0.542	0.181	0.19	0.33	0.126	0.108	0.933	0.507	0.596
RedPajama-INCITE-Instruct-v1 (3B)	0.366	0.257	0.677	0.638	0.203	0.637	0.259	-	-	0.208	-	-	-	-	0.894	0.549	0.661
text-curie-001	0.36	0.237	0.62	0.582	0.175	0.571	0.358	0.676	0.514	0.257	0.271	0.507	0.152	0.076	0.923	0.537	0.489
GPT-NeoX (20B)	0.351	0.276	0.683	0.599	0.193	0.596	0.326	0.718	0.524	0.216	0.184	0.398	0.123	0.102	0.948	0.516	0.505
Luminous Base (13B)	0.315	0.27	0.719	0.605	0.202	0.568	0.334	-	-	0.182	-	-	0.11	0.105	0.939	0.544	0.473
Cohere medium v20221108 (6.1B)	0.312	0.254	0.7	0.61	0.199	0.517	0.314	0.726	0.538	0.215	0.175	0.373	0.121	0.099	0.935	0.5	0.591
RedPajama-INCITE-Base-v1 (3B)	0.311	0.263	0.685	0.555	0.207	0.52	0.309	-	-	0.277	-	-	-	-	0.907	0.549	0.502
TNLG v2 (6.7B)	0.309	0.242	0.698	0.631	0.21	0.561	0.345	0.704	0.478	0.167	0.158	0.332	0.146	0.11	0.927	0.532	0.525
J1-Large v1 (7.5B)	0.285	0.241	0.683	0.623	0.19	0.532	0.328	0.7	0.514	0.197	0.147	0.292	0.134	0.102	0.956	0.532	0.545
GPT-J (6B)	0.273	0.249	0.649	0.545	0.156	0.559	0.33	0.663	0.514	0.199	0.152	0.345	0.131	0.096	0.939	0.52	0.619
Pythia (12B)	0.257	0.274	0.662	0.596	0.175	0.581	0.313	-	-	0.177	-	-	-	-	0.931	0.531	0.514
curie (6.7B)	0.247	0.243	0.656	0.604	0.199	0.552	0.321	0.682	0.502	0.232	0.162	0.3	0.113	0.091	0.889	0.539	0.49
Falcon-Instruct (7B)	0.244	0.275	0.72	0.476	0.194	0.449	0.311	-	-	0.213	-	-	-	-	0.852	0.511	0.523
Cohere medium v20220720 (6.1B)	0.23	0.279	0.659	0.559	0.177	0.504	0.279	0.706	0.496	0.19	0.152	0.374	0.077	0.087	0.935	0.504	0.52
text-babbage-001	0.229	0.229	0.451	0.429	0.07	0.33	0.284	0.561	0.452	0.233	0.208	0.449	0.151	0.046	0.913	0.499	0.509
T0pp (11B)	0.197	0.407	0	0.151	0.039	0.19	0.121	-	-	0.377	-	-	0.122	0.09	0.207	0.234	0.118
Pythia (6.9B)	0.196	0.236	0.631	0.528	0.142	0.539	0.296	-	-	0.213	-	-	-	-	0.928	0.511	0.502
UL2 (20B)	0.167	0.291	0.746	0.083	0.204	0.349	0.144	-	-	0.193	-	-	0.03	0.058	0.337	0.521	0.404
T5 (11B)	0.131	0.29	0.761	0.086	0.194	0.477	0.116	-	-	0.133	-	-	0.043	0.015	0.379	0.509	0.37
babbage (1.3B)	0.114	0.235	0.574	0.491	0.119	0.451	0.273	0.555	0.438	0.188	0.122	0.317	0.079	0.045	0.597	0.519	0.455
Cohere small v20220720 (410M)	0.109	0.264	0.457	0.294	0.078	0.309	0.219	0.483	0.348	0.217	-	0.304	0.063	0.033	0.578	0.501	0.492
ada (350M)	0.108	0.243	0.581	0.326	0.082	0.365	0.242	0.435	0.38	0.215	0.102	0.29	0.09	0.022	0.849	0.517	0.423
text-ada-001	0.107	0.238	0.464	0.238	0.025	0.149	0.176	0.429	0.346	0.232	0.134	0.302	0.136	0.034	0.822	0.503	0.406
YaLM (100B)	0.075	0.243	0.634	0.252	0.068	0.227	0.162	-	-	0.202	-	-	0.017	0.021	0.836	0.49	0.395
